<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta content="width=device-width,initial-scale=1,minimum-scale=1" name="viewport"><meta content="ie=edge" http-equiv="X-UA-Compatible"><meta content="#fff" name="theme-color" id="theme-color"><meta content="My daily record and my safehouse" name="description"><link href="/" rel="icon"><title>实用统计学（三）</title><meta content="实用统计学（三）" property="og:title"><meta content="https://chengziyu.xyz/2022/07/09/statistic/theory/StatisticTheoryWithR3/index.html" property="og:url"><meta content="/assets/img/haibra4.jpg" property="og:img"><meta content="&lt;h2 id=&#34;统计机器学习&#34;&gt;&lt;a class=&#34;header-anchor&#34; href=&#34;#统计机器学习&#34;&gt;&lt;/a&gt;统计机器学习&lt;/h2&gt;
&lt;p&gt;统计机器学习是数据驱动的，并不试图在数据上强加线性结构或其他的整体结构&lt;/p&gt;" property="og:img"><meta content="article" property="og:type"><meta content="2022-07-09" property="og:article:published_time"><meta content="2022-07-19" property="og:article:modified_time"><meta content="是橙子呀" property="og:article:author"><meta content="Statistic" property="og:article:tag"><meta content="R" property="og:article:tag"><link href="//at.alicdn.com/t/font_1946621_i1kgafibvw.css" rel="preload" as="style"><link href="//at.alicdn.com/t/font_1952792_89b4ac4k4up.css" rel="preload" as="style"><link href="/css/main.css" rel="preload" as="style"><link href="//instant.page/5.1.0" rel="modulepreload"><link href="/css/main.css" rel="stylesheet"><link href="//at.alicdn.com/t/font_1946621_i1kgafibvw.css" rel="stylesheet"><link href="//at.alicdn.com/t/font_1952792_89b4ac4k4up.css" rel="stylesheet"><link href="/js/lib/lightbox/baguetteBox.min.css" rel="stylesheet"><script>function loadScript(e,t){var n=document.createElement("script");n.src=e,t&&(n.onload=t),n.async=!0,document.body.appendChild(n)}function loadCSS(e,t,n){var a=document.createElement("link");a.ref="stylesheet",a.href=e,a.dataset[t]=n,document.head.appendChild(a)}function changeCSS(e,t,n){var t=document.querySelector(t),a=document.createElement("link");a.setAttribute("rel","stylesheet"),a.setAttribute("href",e),a.dataset.prism=n,document.head.replaceChild(a,t)}</script><script>var reverseDarkList={dark:"light",light:"dark"},themeColor={dark:"#1c1c1e",light:"#fff"},getCssMediaQuery=function(){return window.matchMedia("(prefers-color-scheme: dark)").matches?"dark":"light"},reverseDarkModeSetting=function(){var e=localStorage.getItem("user-color-scheme");if(reverseDarkList[e])e=reverseDarkList[e];else{if(null!==e)return;e=reverseDarkList[getCssMediaQuery()]}return localStorage.setItem("user-color-scheme",e),e}</script><script>var setDarkmode=function(e){e=e||localStorage.getItem("user-color-scheme");e===getCssMediaQuery()?(document.documentElement.removeAttribute("data-user-color-scheme"),localStorage.removeItem("user-color-scheme"),document.getElementById("theme-color").content=themeColor[e],document.getElementById("theme-color").dataset.mode=e):reverseDarkList[e]?(document.documentElement.setAttribute("data-user-color-scheme",e),document.getElementById("theme-color").content=themeColor[e],document.getElementById("theme-color").dataset.mode=e):(document.documentElement.removeAttribute("data-user-color-scheme"),localStorage.removeItem("user-color-scheme"),document.getElementById("theme-color").content=themeColor[getCssMediaQuery()],document.getElementById("theme-color").dataset.mode=getCssMediaQuery()),prismThemeChange()};setDarkmode()</script><link href="/js/lib/lightbox/baguetteBox.min.js" rel="preload" as="script"><link href="/js/lib/lightbox/baguetteBox.min.css" rel="preload" as="style"><link href="/js/lib/lozad.min.js" rel="preload" as="script"><link href="//cdn.jsdelivr.net/npm/mathjax@3.0.5/es5/tex-svg.js" rel="prefetch" as="script"><meta content="Hexo 5.4.2" name="generator"><style>.github-emoji{position:relative;display:inline-block;width:1.2em;min-height:1.2em;overflow:hidden;vertical-align:top;color:transparent}.github-emoji>span{position:relative;z-index:10}.github-emoji .fancybox,.github-emoji img{margin:0!important;padding:0!important;border:none!important;outline:0!important;text-decoration:none!important;user-select:none!important;cursor:auto!important}.github-emoji img{height:1.2em!important;width:1.2em!important;position:absolute!important;left:50%!important;top:50%!important;transform:translate(-50%,-50%)!important;user-select:none!important;cursor:auto!important}.github-emoji-fallback{color:inherit}.github-emoji-fallback img{opacity:0!important}</style></head><body><div class="wrapper"><nav class="navbar"><div class="navbar-logo"><a href="/" class="navbar-logo-main"><span class="navbar-logo-dsc">橙子安全屋</span></a></div><div class="navbar-menu"><a href="/" class="navbar-menu-item">首页 </a><a href="/archives" class="navbar-menu-item">归档 </a><a href="/tags" class="navbar-menu-item">标签 </a><a href="/categories" class="navbar-menu-item">分类 </a><a href="/about" class="navbar-menu-item">关于 </a><a href="/links" class="navbar-menu-item">友链 </a><button aria-label="Toggle dark mode" class="navbar-menu-item navbar-menu-btn darknavbar" id="dark"><i class="iconfont icon-weather"></i></button> <button aria-label="Toggle search" class="navbar-menu-item navbar-menu-btn searchnavbar" id="search"><!-- <i 
        class="iconfont icon-search" 
        style="font-size: 1.2rem; font-weight: 400;">
      </i> --> <svg aria-hidden="true" class="iconify iconify--ion" height="28" preserveAspectRatio="xMidYMid meet" role="img" viewBox="0 0 512 512" width="28" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M256 80a176 176 0 1 0 176 176A176 176 0 0 0 256 80Z" fill="none" stroke="currentColor" stroke-miterlimit="10" stroke-width="28"></path><path d="M232 160a72 72 0 1 0 72 72a72 72 0 0 0-72-72Z" fill="none" stroke="currentColor" stroke-miterlimit="10" stroke-width="28"></path><path d="M283.64 283.64L336 336" fill="none" stroke="currentColor" stroke-miterlimit="10" stroke-width="28" stroke-linecap="round"></path></svg></button></div></nav><div id="local-search" style="display:none"><input class="navbar-menu-item" id="search-input" placeholder="请输入搜索内容..."><div id="search-content"></div></div><div class="section-wrap"><div class="container"><div class="columns"><aside class="left-column"><div class="card card-author"><img alt="author avatar" class="author-img" height="88" src="/assets/img/haibra4.jpg" width="88"><p class="author-name">是橙子呀</p><p class="author-description">橙子碎碎念</p><div class="author-message"><a href="/archives" class="author-posts-count"><span>39</span> <span>文章</span> </a><a href="/categories" class="author-categories-count"><span>15</span> <span>分类</span> </a><a href="/tags" class="author-tags-count"><span>28</span> <span>标签</span></a></div></div><div class="sticky-tablet"><article class="display-when-two-columns spacer"><div class="card card-content toc-card"><div class="toc-header"><i class="iconfont icon-menu" style="padding-right:2px"></i>目录</div><ol class="toc"><li class="toc-item toc-level-2"><a href="#%E7%BB%9F%E8%AE%A1%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0" class="toc-link"><span class="toc-number">1.</span> <span class="toc-text">统计机器学习</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a href="#K%E6%9C%80%E8%BF%91%E9%82%BB%E7%AE%97%E6%B3%95" class="toc-link"><span class="toc-number">1.1.</span> <span class="toc-text">K最近邻算法</span></a></li><li class="toc-item toc-level-3"><a href="#%E6%A0%91%E6%A8%A1%E5%9E%8B" class="toc-link"><span class="toc-number">1.2.</span> <span class="toc-text">树模型</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a href="#Bagging%E6%96%B9%E6%B3%95" class="toc-link"><span class="toc-number">1.2.1.</span> <span class="toc-text">Bagging方法</span></a></li><li class="toc-item toc-level-4"><a href="#%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97" class="toc-link"><span class="toc-number">1.2.2.</span> <span class="toc-text">随机森林</span></a></li><li class="toc-item toc-level-4"><a href="#Boosting" class="toc-link"><span class="toc-number">1.2.3.</span> <span class="toc-text">Boosting</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a href="#%E9%AB%98%E6%B0%8F%E8%B7%9D%E7%A6%BB%EF%BC%88Gower%E2%80%99s-distance%EF%BC%89" class="toc-link"><span class="toc-number">1.2.3.1.</span> <span class="toc-text">高氏距离（Gower’s distance）</span></a></li><li class="toc-item toc-level-5"><a href="#%E6%B7%B7%E5%90%88%E6%95%B0%E6%8D%AE%E7%9A%84%E8%81%9A%E7%B1%BB%E9%97%AE%E9%A2%98" class="toc-link"><span class="toc-number">1.2.3.2.</span> <span class="toc-text">混合数据的聚类问题</span></a></li></ol></li></ol></li></ol></li></ol></div></article><article class="card card-content categories-widget"><div class="categories-card"><div class="categories-header"><i class="iconfont icon-fenlei" style="padding-right:2px"></i>分类</div><div class="categories-list"><a href="/categories/Daily-Record/"><div class="categories-list-item">Daily-Record <span class="categories-list-item-badge">16</span></div></a><a href="/categories/Code/"><div class="categories-list-item">Code <span class="categories-list-item-badge">18</span></div></a><a href="/categories/Daily-Record/Technology/"><div class="categories-list-item">Daily-Record/Technology <span class="categories-list-item-badge">12</span></div></a><a href="/categories/Code/BD/"><div class="categories-list-item">Code/BD <span class="categories-list-item-badge">10</span></div></a><a href="/categories/Code/DataScience/"><div class="categories-list-item">Code/DataScience <span class="categories-list-item-badge">4</span></div></a><a href="/categories/Code/Algorithm/"><div class="categories-list-item">Code/Algorithm <span class="categories-list-item-badge">1</span></div></a><a href="/categories/Code/shell/"><div class="categories-list-item">Code/shell <span class="categories-list-item-badge">3</span></div></a><a href="/categories/Daily-Record/Notes/"><div class="categories-list-item">Daily-Record/Notes <span class="categories-list-item-badge">3</span></div></a><a href="/categories/Daily-Record/novel/"><div class="categories-list-item">Daily-Record/novel <span class="categories-list-item-badge">1</span></div></a><a href="/categories/git/"><div class="categories-list-item">git <span class="categories-list-item-badge">1</span></div></a><a href="/categories/Linux/"><div class="categories-list-item">Linux <span class="categories-list-item-badge">1</span></div></a><a href="/categories/Markdown/"><div class="categories-list-item">Markdown <span class="categories-list-item-badge">1</span></div></a><a href="/categories/shell/"><div class="categories-list-item">shell <span class="categories-list-item-badge">1</span></div></a><a href="/categories/Statistic/"><div class="categories-list-item">Statistic <span class="categories-list-item-badge">5</span></div></a><a href="/categories/Statistic/Theory/"><div class="categories-list-item">Statistic/Theory <span class="categories-list-item-badge">5</span></div></a></div></div></article><article class="card card-content tags-widget"><div class="tags-card"><div class="tags-header"><i class="iconfont icon-biaoqian" style="padding-right:2px"></i>热门标签</div><div class="tags-list"><a href="/tags/Linux/" title="Linux"><div class="tags-list-item">Linux</div></a><a href="/tags/BD/" title="BD"><div class="tags-list-item">BD</div></a><a href="/tags/Statistic/" title="Statistic"><div class="tags-list-item">Statistic</div></a><a href="/tags/R/" title="R"><div class="tags-list-item">R</div></a><a href="/tags/Python/" title="Python"><div class="tags-list-item">Python</div></a><a href="/tags/SQL/" title="SQL"><div class="tags-list-item">SQL</div></a><a href="/tags/%E6%95%B0%E6%8D%AE%E6%B8%85%E6%B4%97/" title="数据清洗"><div class="tags-list-item">数据清洗</div></a><a href="/tags/DataScience/" title="DataScience"><div class="tags-list-item">DataScience</div></a><a href="/tags/Algorithm/" title="Algorithm"><div class="tags-list-item">Algorithm</div></a><a href="/tags/Docker/" title="Docker"><div class="tags-list-item">Docker</div></a><a href="/tags/Spark/" title="Spark"><div class="tags-list-item">Spark</div></a><a href="/tags/Hexo/" title="Hexo"><div class="tags-list-item">Hexo</div></a><a href="/tags/nginx/" title="nginx"><div class="tags-list-item">nginx</div></a><a href="/tags/Vim/" title="Vim"><div class="tags-list-item">Vim</div></a><a href="/tags/Windows/" title="Windows"><div class="tags-list-item">Windows</div></a><a href="/tags/Markdown/" title="Markdown"><div class="tags-list-item">Markdown</div></a></div></div></article></div></aside><main class="main-column"><article class="card card-content"><header><h1 class="post-title">实用统计学（三）</h1></header><div class="post-meta post-show-meta"><time datetime="2022-07-09T09:30:20.000Z"><i class="iconfont icon-calendar" style="margin-right:2px"></i> <span>2022-07-09</span> </time><span class="dot"></span> <a href="/categories/Statistic/" class="post-meta-link">Statistic </a><a href="/categories/Statistic/Theory/" class="post-meta-link">Theory </a><span class="dot"></span> <span>5.1k 字</span></div><div class="post-meta post-show-meta" style="margin-top:-10px"><div style="display:flex;align-items:center"><i class="iconfont icon-biaoqian" style="margin-right:2px;font-size:1.15rem"></i> <a href="/tags/Statistic/" class="post-meta-link">Statistic </a><span class="dot"></span> <a href="/tags/R/" class="post-meta-link">R</a></div></div><div class="post-content" id="section"><h2 id="统计机器学习"><a href="#统计机器学习" class="header-anchor"></a>统计机器学习</h2><p>统计机器学习是数据驱动的，并不试图在数据上强加线性结构或其他的整体结构</p><span id="more"></span><h3 id="K最近邻算法"><a href="#K最近邻算法" class="header-anchor"></a>K最近邻算法</h3><p>对于每个要进行分类或预测的记录，该算法：<br>(1) 找出 K 个具有相似特征（即具有相似的预测值）的记录。<br>(2) 对于分类，找出这些相似记录中的多数类，将其指定为新记录的类。<br>(3) 对于预测（KNN回归），找出这些相似记录的均值，并将该均值作为新记录的预测值。<br>K 最近邻算法的预测结果取决于特征的规模、相似性的测定方法以及 K 值的设置等因素。此外在K最近邻算法中，所有的预测变量必须是数值型的。</p><p>近邻——具有相似预测值的两个记录。<br>距离度量——以单一数值的形式，测量两个记录之间的距离。<br>标准化（归一化）——减去均值，并除以标准偏差。<br>z分数——标准化后得到的值。<br>K——在最近邻计算中考虑的近邻个数。<br>　<br>1.　距离度量<br>距离度量用于判定相似性（接近度），它是一个测量两个记录之间距离的函数。最广为使用的向量距离度量是欧氏距离。<br>在测量两个向量间的欧氏距离时，依次取两个向量中对应元素的差值，并对各个差值平方，累加后再取平方根<br>$\sqrt{(x_1-u_1)^2+ \dots+(x_p-u_p)^2}$<br>如果数据是数值型的，那么另一种常用的距离度量是曼哈顿距离<br>$|x_1-u_1| + \dots + |x_p-u_p|$</p><ul><li>欧氏距离表示的是两点之间的直线距离</li><li>曼哈顿距离是在某一时刻以同一方向遍历两点之间的距离<ul><li>如遍历矩形城市街区,如果定义相似性为点到点的行程时间，那么曼哈顿距离更适用。欧氏距离和曼哈顿距离并未考虑相关性，只是对这些特征的属性添加了更大的权重</li><li>在测量两个向量之间的距离时，测量值取决于规模相对较大的变量（特征）。马氏距离的缺点是在计算中要使用协方差矩阵，这增加了计算的难度和复杂性</li></ul></li></ul><ol><li><p>独热编码</p></li><li><p>标准化<br>归一化、标准化<br>z 分数只是一种重新调整变量尺度的方式。均值可以替换成更稳健的位置估计量（中位数）<br>数据库标准化的目的在于去除冗余的数据，并验证数据的依赖关系</p></li></ol><p>K 值设置过高，会对数据做过平滑，进而丧失了KNN的一个主要优点，即捕获数据局部结构的能力。KNN 本身在性能上并不具有竞争力。在实际的模型拟合中，可以将 KNN 作为一个阶段性过程，用于向其他分类方法中添加“局部知识”。具体做法如下：<br>(1) 在数据上运行 KNN，为每个记录生成一个分类（或是分类的拟概率）；<br>(2) 将结果作为一个新特征添加到记录中，然后在生成的数据上运行另一种分类方法。这样使用了原始预测变量两次。</p><ul><li>预测变量通常需要做标准化，以避免大尺度变量主导了距离度量。</li><li>KNN 常常作为预测建模过程的第一个阶段。KNN 的预测值会作为一个预测变量添加回数据中，进而用于第二阶段（非 KNN）的建模</li></ul><h3 id="树模型"><a href="#树模型" class="header-anchor"></a>树模型</h3><p>简单树模型可以表示为预测变量之间的关系，易于解释</p><table><thead><tr><th>概念</th><th>定义</th></tr></thead><tbody><tr><td>递归分区</td><td>反复对数据进行划分和细分，目的是使每个最终细分内的结果尽可能同质。（recursive partition）</td></tr><tr><td>拆分值</td><td>一个预测变量值，它将一组记录分为两部分，使得一部分中的预测变量小于拆分值，而另一部分中的预测变量大于拆分值。（split value）</td></tr><tr><td>节点</td><td>在决策树中（或在一组相应的分支规则中），节点是拆分值的图形化表示（或规则表示）。</td></tr><tr><td>叶子</td><td>一组 if-then 规则的终点，或一个树分支的终点。在树中访问叶子的规则，构成了对树中一条记录的分类规则。</td></tr><tr><td>损失</td><td>在拆分过程的某一阶段中误分类的个数。损失越大，不纯度越高。</td></tr><tr><td>不纯度(异质性)</td><td>表示在数据的一个细分中发现多个类混杂的程度。细分中混杂的类越多，该细分的不纯度就越高。(反义词：同质性、纯度)</td></tr><tr><td>剪枝</td><td>为了降低过拟合，对一棵完全长成树逐步剪枝的过程。</td></tr></tbody></table><blockquote><p>拟合树模型主要使用软件包 rpart 和 tree</p></blockquote><p>1.　递归分区算法<br>决策树的构造算法被称为递归分区法</p><ol><li>测量同质性或不纯度<br>树模型递归地创建分区（记录的集合），并给出 Y = 0 或 Y = 1的预测结果。从递归算法中可以看到，还需要一种测量分区同质性（也称为类纯度）的方法。或者也可以说，需要测量分区的不纯度。预测的正确率是分区内误分类记录的比例 p。p 的取值介于0（即完美分区）和 0.5（即纯随机猜测）之间。<br>对于不纯度来说，正确率并非一种很好的度量。<br>两种常用的不纯度度量分别是基尼不纯度和熵（或信息）</li></ol><ul><li>基尼不纯度</li><li>熵 $I(A)= -plog_2§-(1-p)log_2(1-p)$</li></ul><p>基尼系数仅限于二分类问题，并与 AUC 度量有关</p><ol><li>剪枝<br>剪枝到验证数据集上的误差达到最小<br>停止拆分<ul><li>如果拆分后的子分区过小，或末端叶子的规模过小，就应避免拆分。在 rpart 中，这些约束是由参数 minsplit 和 minbucket 控制的，默认值分别是 20和 7</li><li>如果新分区并未使不纯度“显著”降低，那么就不必拆分该分区。在 rpart 中，这由复杂度参数 cp 控制。该参数度量了树模型的复杂度。树模型越复杂，cp 的值就越大。在实践中，参数cp是通过对树模型的额外复杂度（拆分）附加惩罚项，从而限制了树模型的增长</li></ul></li></ol><p>最常用的 cp 值估计方法是使用交叉验证<br>(1) 将数据分为训练集和验证集。<br>(2) 使用训练集生长树模型。<br>(3) 逐步剪枝，并在每步记录 cp 值（使用训练集）。<br>(4) 注意在验证集上取得最小误差（损失）的 cp 值。<br>(5) 将数据重新拆分为训练集和验证集，并重复树模型的生长、剪枝和记录 cp 过程。<br>(6) 重复执行上述步骤，对反映每个树的最小误差的 cp 值求平均值。<br>(7) 回到原始数据，也可以是将要处理的数据上，生长树模型，并在最优 cp 值处终止执行算法。<br>1.　预测连续值<br>使用树模型预测连续值（也被称为回归）时，遵循着同样的逻辑和过程，不同之处是使用每个子分区中距离均值的平方偏差（平方误差）来度量不纯度，并通过每个分区中的均方误差的平方根判断预测性能</p><ul><li>完全长成树会过产生拟合，因此为了使模型捕获信号而非噪声，必须做剪枝。</li><li>虽然随机森林和 Boosting 等多树模型算法具有更好的预测性能，但失去了单个树模型基于规则的交流能力。</li></ul><p>###　Bagging和随机森林<br>集成（模型平均）<br>　　使用一组模型给出预测。<br>　　主要变体——bagging 、Boosting<br>Bagging（自助法聚合）<br>　　对数据使用自助法构建一组模型的通用方法。<br>随机森林（自助法聚合决策树）<br>　　使用决策树的一类自助法聚合估计。<br>变量重要性<br>　　对预测变量在模型性能中重要性的测量</p><h4 id="Bagging方法"><a href="#Bagging方法" class="header-anchor"></a>Bagging方法</h4><p>集成方法的基本实现<br>(1) 给定一个数据集，采用一种预测模型，并记录该模型的预测情况。<br>(2) 在同一数据集上，依次使用多个模型重复步骤 1。<br>(3) 对于每个要预测的记录，对预测值取均值（或加权均值，也可以使用多数票）</p><p>Bagging 方法和集成的基本算法类似，只是 Bagging 并不是要在同一数据上拟合所有的模型，而是对使用每个自助法重抽样拟合一个新模型。</p><h4 id="随机森林"><a href="#随机森林" class="header-anchor"></a>随机森林</h4><p>随机森林是将 Bagging 方法应用于决策树，并做了一个重要的扩展。该算法不仅对记录做抽样，而且也对变量做抽样。<br>传统的决策树在确定如何将一个分区A拆分为子分区时，通过最小化基尼不纯度等标准去选择变量和拆分点。在随机森林算法中，每一阶段的变量选择受限于变量的一个随机子集。与基本的树算法相比，随机森林算法额外添加了两步，分别是Bagging方法，以及每次拆分时对变量的自助法抽样</p><p>(1) 从记录中做一次自助法（带放回的）抽样，得到一个子样本。<br>(2) 对于第一次拆分，无放回地随机抽样 p(p &lt; P)个变量。<br>(3) 对于每组抽样变量 ，应用如下的拆分算法。<br>　　a.　对于$X_{j(k)}$ 的每个值$s_{j(k)}$ ： 将分区 A 中满足$X_{j(k)}\leq s_{j(k)}$ 的记录拆分为一个分区，其余满足$X_{j(k)}&gt;= s_{j(k)}$的记录作为另一个分区；测量 A 的每个子分区中类的同质性。<br>　　b.　选择生成分区内最大类同质性的$X_{j(k)}$。<br>(4) 选择生成分区内最大类同质性的变量$X_{j(k)}$和拆分值$X_{j(k)}$ 。<br>(5) 继续下一次拆分，重复从步骤 2 开始的上述步骤。<br>(6) 遵循同一过程，继续拆分，直到得到一棵完全长成树。<br>(7) 返回步骤 1，再做一次自助法抽样，得到子样本，并重复上述过程</p><p>每一步需要抽样的变量数根据经验规则是选取$\sqrt{p}$个，其中 P 是预测变量的个数。</p><p>在为具有多个特征和记录的数据构建预测模型时，随机森林算法的强大得以尽显。<br>该算法可以自动确定重要的预测变量，并可以发现交互项所对应的预测变量之间的复杂关系</p><p>误差的包外（OOB）估计是指将训练得到的模型作用于训练集中未使用的数据上时，所得到的错误率。使用模型的输出，可以绘制出 OOB 误差与随机森林中树模型的数量</p><ol><li>超参数<br>参数是一些需要在拟合模型前设置的参数，它们并不会在算法训练过程中得到优化。<br>随机森林具有一组超参数。可以使用交叉验证调整超参数，以避免产生过拟合。</li></ol><p>nodesize<br>　　末端节点（即树的叶子）的最小规模。对于分类，默认值为 1；对于回归，默认值为 5。<br>maxnodes<br>　　每个决策树中的最大节点数。默认情况下，没有限制。<br>需拟合的最大树会受到nodesize 的限制。当增大 nodesize 或设置maxnodes后，算法将拟合一个较小的树模型，而且不太可能给出假的预测规则。可以使用交叉验证），检验设置不同超参数值的效果。</p><h4 id="Boosting"><a href="#Boosting" class="header-anchor"></a>Boosting</h4><p>1.　Boosting算法</p><table><thead><tr><th>概念</th><th>定义</th></tr></thead><tbody><tr><td>Boosting</td><td>在拟合一组模型时所使用的一种通用方法。Boosting 在每轮连续的拟合中，会对具有更大残差的记录赋予更大的权重。</td></tr><tr><td>Adaboost</td><td>Boosting 算法的一种早期实现，它根据残差的情况对数据重新加权。</td></tr><tr><td>梯度提升</td><td>一种更通用的 Boosting 算法。它将问题转化为代价函数最小化的问题。</td></tr><tr><td>随机梯度提升（SGD）</td><td>最常用的 Boosting 算法。它在每轮拟合中加入了对记录和数据列的重抽样</td></tr><tr><td>1.　XGBoost</td><td></td></tr><tr><td>软件包中的函数 xgboost 提供了多个可调整并且应该调整的两个非常重要的参数是 subsample 和 eta。</td><td></td></tr><tr><td>subsample 参数控制每次迭代时应该被抽样的部分观测值</td><td></td></tr><tr><td>eta 设置了 Boosting 算法中 的收缩因子</td><td></td></tr></tbody></table><ul><li>xgboost 不支持公式语法，因此需要将预测变量转换为 R 语言的 data.matrix 对象</li><li>响应变量需要转换为 0/1 二元变量</li><li>参数 objective 指定了 xgboost 函数所处理的问题类型</li><li>xgboost 根据该参数选取优化指标</li></ul><ol><li><p>正则化：避免过拟合<br>对模型的复杂性添加惩罚项，有助于避免产生过拟合。最小二乘回归会最小化残差平方和（RSS）。而岭回归则最小化残差平方和，并对系数的数量和大小添加惩罚项</p></li><li><p>超参数和交叉验证<br>交叉验证指出，使用了较小的eta值并且层数较少的树模型，会得出更准确的结果。由于这样的模型也更稳定，所以应使用的最佳参数是 eta = 0.1，max_depth = 3（也可能是max_depth = 6）</p></li></ol><table><thead><tr><th>arg</th><th>meaning</th></tr></thead><tbody><tr><td>eta</td><td>值位于 0 和 1 之间的收缩因子，即 Boosting 算法的 α。默认值为 0.3。但是对于噪声数据，推荐使用更小的值，例如 0.1。</td></tr><tr><td>nrounds</td><td>设置 Boosting 算法的循环次数。如果将eta设置为一个较小的值，这会增加循环的次数，因为算法学习的速度更慢。只要在计算中设置了一些防止过拟合的参数，那么运行更多轮循环也没问题。</td></tr><tr><td>max_depth</td><td>设置树模型的最大深度，默认值为 6。与随机森林拟合非常深的树模型不同，Boosting 算法通常会拟合一个层数不多的树模型。这样做的优点是，可以避免由噪声数据导致模型中出现虚假的复杂交互。</td></tr><tr><td>subsample 和 colsample_bytree</td><td></td></tr><tr><td>subsample 指定了做无放回抽样的部分记录，colsample_bytree指定了在拟合树模型中要抽样的部分预测变量。这些参数类似于随机森林中使用的相应参数，有助于避免产生过拟合。</td><td></td></tr><tr><td>lambda 和 alpha</td><td></td></tr><tr><td>帮助控制过拟合的正则化参数,分别表示曼哈顿距离和欧氏距离的平方,增大这两个参数将会惩罚更复杂的模型，并减小拟合树模型的规模。</td><td></td></tr><tr><td>###　无监督学习</td><td></td></tr><tr><td>####　主成分分析</td><td></td></tr><tr><td>主成分</td><td></td></tr><tr><td>预测变量的一种线性组合。</td><td></td></tr><tr><td>载荷（权重）</td><td></td></tr><tr><td>将预测因子转换为成分的过程中所使用的权重值</td><td></td></tr><tr><td>陡坡图（碎石图）</td><td></td></tr><tr><td>一种展示各成分方差的绘图，图中显示了各成分的相对重要性主成分分析的基本理念是，将多个数值型预测变量组合成一组规模较小的变量，它们是原始变量的加权线性组合。所形成的规模较小的一组变量被称为主成分。</td><td></td></tr><tr><td>主成分可以“解释”完整变量集的大部分变异性，同时降低数据维度。在构建主成分中所使用的权重，体现了原始变量对新的主成分的相对贡献</td><td></td></tr><tr><td>在 R 语言中，可以使用函数 princomp 计算主成分</td><td></td></tr><tr><td>1.　计算主成分</td><td></td></tr><tr><td>1.　解释主成分</td><td></td></tr></tbody></table><p>####　K-Means聚类</p><table><thead><tr><th>概念</th><th>定义</th></tr></thead><tbody><tr><td>类（cluster）</td><td>一组类似的记录。</td></tr><tr><td>类均值</td><td>表示类内记录变量均值的向量。</td></tr><tr><td>K</td><td>类的个数</td></tr><tr><td>一个类内每个记录到该类均值之间距离的平方和为类内平方和</td><td></td></tr><tr><td>K-Means 通过最小化类内平方和，将数据划分为 K 个类。</td><td></td></tr><tr><td>K-Means 并不能保证各个类的规模相同，但是能找出相互分离情况最好的类。</td><td></td></tr><tr><td>应对连续变量做归一化（标准化），一般做法是减去均值再除以标准偏差</td><td></td></tr><tr><td>类均值（cluster mean）并非指单个数字，而是指表示变量均值的向量。</td><td></td></tr></tbody></table><p>####　层次聚类（系统聚类）<br>层次聚类比 K-Means 更灵活，并且更易于应用在非数值型变量上，对于发现离群的或异常的组和记录也更为敏感<br>层次聚类也适于做直观的图形展示，因而解释类也更为容易<br>但不能很好地扩展到具有数百万条记录的大规模数据集，层次聚类的大部分应用都集中在一些规模相对较小的数据集上</p><ul><li>层次聚类算法开始时，每条记录单独构成一个类。</li><li>在凝聚算法中，类逐步与相邻的类合并，直到所有记录属于单一类。<br>-凝聚算法的类历史可以被保留并绘制出来。用户（无须预先指定类数）可以在算法执行的各个阶段，可视化地查看类数和类的结构。</li><li>有多种方法可以计算类之间的距离。这些方法都依赖于所有记录间距离</li></ul><p>####　基于模型的聚类<br>最广为使用的基于模型的聚类方法依赖于多元正态分布<br>基于模型的聚类的关键思想是，假定每条记录的分布符合 K 个多元正态分布之一，其中 K 是类的个数。每个分布具有不同的均值$\mu$和协方差矩阵$\sum$<br>R 语言的 mclust 软件包提供了丰富的基于模型聚类的功能。</p><p>不同于 K-Means 和层次聚类，mclust 会自动选取类数</p><p>通过选取使贝叶斯信息准则（BIC）值最大的类数实现的。BIC（类似于 AIC）是在一组候选模型中找到最佳模型的通用工具。<br>例如，AIC（或 BIC）常用于在逐步回归中选择模型。BIC通过对模型中的参数数量添加一个惩罚项，选择最优拟合模型。在基于模型的聚类中，增加类数总是会提高模型的拟合度，但代价是在模型中额外地引入了一些参数</p><p>####　变量的缩放和分类变量</p><h5 id="高氏距离（Gower’s-distance）"><a href="#高氏距离（Gower’s-distance）" class="header-anchor"></a>高氏距离（Gower’s distance）</h5><p>一种应用于数值数据和类别数据相混合的缩放算法。它可以将所有变量缩放到 [0, 1]范围内<br>高氏距离的基本思想是，根据数据类型，对每个变量应用不同的距离度量。</p><ul><li>对于数值型变量和有序因子，高氏距离计算为两条记录间差异的绝对值（即曼哈顿距离）。</li><li>对于分类变量，如果两个记录属于不同的类，那么距离为 1；如果它们属于同一个分类，那么距离为 0。</li></ul><p>流程</p><ol><li>对每条记录，计算所有变量对 i 和 j 间的距离$d_{ij}$</li><li>将每个距离 缩放到区间 [0, 1] 中。</li><li>使用简单均值或加权均值，将所有变量对间的缩放距离相加，创建一个距离矩阵。</li></ol><h5 id="混合数据的聚类问题"><a href="#混合数据的聚类问题" class="header-anchor"></a>混合数据的聚类问题</h5><p>K-Means 和主成分分析最适合用于连续变量。<br>对于较小的数据集，使用基于高氏距离的层次聚类更好。<br>从原理上看，K-Means 完全适用于二元数据和分类数据。通常会使用“独热编码”，将分类数据转换为数值型数据。<mark>然而在实践中，很难对二元数据应用K-Means 和主成分分析。</mark><br>如果使用标准的 z 分数，那么二元变量将会主导聚类的定义。这是因为 0/1 变量只有两个值，<br>而 K-Means 是通过将所有的 0 或 1 记录指定给聚类，获得较小的类内平方和</p></div><div><div class="copyright note-warning post-note" style="margin-top:42px"><p><span style="font-weight:700">作者：</span><a href="/about" rel="nofollow noopener noreferrer" target="_blank"> 是橙子呀</a></p><p><span style="font-weight:700">文章链接：</span><a href="https://chengziyu.xyz/2022/07/09/statistic/theory/StatisticTheoryWithR3/" rel="nofollow noopener noreferrer" target="_blank"> https://chengziyu.xyz/2022/07/09/statistic/theory/StatisticTheoryWithR3/</a></p><p><span style="font-weight:700">版权声明：</span>本博客所有文章除特别声明外，均采用<a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh" rel="noopener" target="_blank">CC BY-NC-SA 4.0 协议</a>。转载请注明出处！</p></div></div></article><div class="nav"><div class="nav-item-prev"><a href="/2022/07/09/daily-record/technology/CommandLineTools/" class="nav-link"><i class="iconfont icon-left nav-prev-icon"></i><div><div class="nav-label">上一篇</div><div class="nav-title">命令行工具</div></div></a></div><div class="nav-item-next"><a href="/2022/07/07/code/DataScience/Data_Cleaning/" class="nav-link"><div><div class="nav-label">下一篇</div><div class="nav-title">数据清洗(1)</div></div><i class="iconfont icon-right nav-next-icon"></i></a></div></div><div class="card card-content toc-card" id="mobiletoc"><div class="toc-header"><i class="iconfont icon-menu" style="padding-right:2px"></i>目录</div><ol class="toc"><li class="toc-item toc-level-2"><a href="#%E7%BB%9F%E8%AE%A1%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0" class="toc-link"><span class="toc-number">1.</span> <span class="toc-text">统计机器学习</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a href="#K%E6%9C%80%E8%BF%91%E9%82%BB%E7%AE%97%E6%B3%95" class="toc-link"><span class="toc-number">1.1.</span> <span class="toc-text">K最近邻算法</span></a></li><li class="toc-item toc-level-3"><a href="#%E6%A0%91%E6%A8%A1%E5%9E%8B" class="toc-link"><span class="toc-number">1.2.</span> <span class="toc-text">树模型</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a href="#Bagging%E6%96%B9%E6%B3%95" class="toc-link"><span class="toc-number">1.2.1.</span> <span class="toc-text">Bagging方法</span></a></li><li class="toc-item toc-level-4"><a href="#%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97" class="toc-link"><span class="toc-number">1.2.2.</span> <span class="toc-text">随机森林</span></a></li><li class="toc-item toc-level-4"><a href="#Boosting" class="toc-link"><span class="toc-number">1.2.3.</span> <span class="toc-text">Boosting</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a href="#%E9%AB%98%E6%B0%8F%E8%B7%9D%E7%A6%BB%EF%BC%88Gower%E2%80%99s-distance%EF%BC%89" class="toc-link"><span class="toc-number">1.2.3.1.</span> <span class="toc-text">高氏距离（Gower’s distance）</span></a></li><li class="toc-item toc-level-5"><a href="#%E6%B7%B7%E5%90%88%E6%95%B0%E6%8D%AE%E7%9A%84%E8%81%9A%E7%B1%BB%E9%97%AE%E9%A2%98" class="toc-link"><span class="toc-number">1.2.3.2.</span> <span class="toc-text">混合数据的聚类问题</span></a></li></ol></li></ol></li></ol></li></ol></div></main><aside class="right-column"><div class="sticky-widescreen"><article class="card card-content toc-card"><div class="toc-header"><i class="iconfont icon-menu" style="padding-right:2px"></i>目录</div><ol class="toc"><li class="toc-item toc-level-2"><a href="#%E7%BB%9F%E8%AE%A1%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0" class="toc-link"><span class="toc-number">1.</span> <span class="toc-text">统计机器学习</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a href="#K%E6%9C%80%E8%BF%91%E9%82%BB%E7%AE%97%E6%B3%95" class="toc-link"><span class="toc-number">1.1.</span> <span class="toc-text">K最近邻算法</span></a></li><li class="toc-item toc-level-3"><a href="#%E6%A0%91%E6%A8%A1%E5%9E%8B" class="toc-link"><span class="toc-number">1.2.</span> <span class="toc-text">树模型</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a href="#Bagging%E6%96%B9%E6%B3%95" class="toc-link"><span class="toc-number">1.2.1.</span> <span class="toc-text">Bagging方法</span></a></li><li class="toc-item toc-level-4"><a href="#%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97" class="toc-link"><span class="toc-number">1.2.2.</span> <span class="toc-text">随机森林</span></a></li><li class="toc-item toc-level-4"><a href="#Boosting" class="toc-link"><span class="toc-number">1.2.3.</span> <span class="toc-text">Boosting</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a href="#%E9%AB%98%E6%B0%8F%E8%B7%9D%E7%A6%BB%EF%BC%88Gower%E2%80%99s-distance%EF%BC%89" class="toc-link"><span class="toc-number">1.2.3.1.</span> <span class="toc-text">高氏距离（Gower’s distance）</span></a></li><li class="toc-item toc-level-5"><a href="#%E6%B7%B7%E5%90%88%E6%95%B0%E6%8D%AE%E7%9A%84%E8%81%9A%E7%B1%BB%E9%97%AE%E9%A2%98" class="toc-link"><span class="toc-number">1.2.3.2.</span> <span class="toc-text">混合数据的聚类问题</span></a></li></ol></li></ol></li></ol></li></ol></article><article class="card card-content"><div class="recent-posts-card"><div class="recent-posts-header"><i class="iconfont icon-wenzhang_huaban" style="padding-right:2px"></i>最近文章</div><div class="recent-posts-list"><div class="recent-posts-item"><div class="recent-posts-item-title">2022-07-22</div><a href="/2022/07/22/daily-record/technology/DockerRecord/"><div class="recent-posts-item-content">Docker记录</div></a></div><div class="recent-posts-item"><div class="recent-posts-item-title">2022-07-22</div><a href="/2022/07/22/daily-record/technology/LetsencryptRecord/"><div class="recent-posts-item-content">Certbot证书相关记录</div></a></div><div class="recent-posts-item"><div class="recent-posts-item-title">2022-07-22</div><a href="/2022/07/22/daily-record/technology/port_errors_record/"><div class="recent-posts-item-content">端口相关</div></a></div><div class="recent-posts-item"><div class="recent-posts-item-title">2022-07-20</div><a href="/2022/07/20/code/BigData/hive-1/"><div class="recent-posts-item-content">Conception Of Hive</div></a></div></div></div></article></div></aside></div></div></div></div><footer class="footer"><div class="footer-container"><div><div class="footer-dsc"><span>Copyright © 2022 </span>&nbsp; <a href="/" class="footer-link">橙子安全屋</a></div></div><div class="footer-dsc">Powered by <a href="https://hexo.io/" class="footer-link" rel="nofollow noopener noreferrer" target="_blank">&nbsp;Hexo</a></div><div class="footer-dsc">本站总访问量<span id="busuanzi_value_site_pv"></span>次 <span>&nbsp;|&nbsp;</span> 本站总访客数<span id="busuanzi_value_site_uv"></span>次</div></div></footer><a class="basebutton" aria-label="回到顶部" id="scrollbutton" role="button"><i class="iconfont button-icon icon-arrowleft"></i> </a><a class="basebutton" aria-label="menu button" id="menubutton" role="button"><i class="iconfont button-icon icon-menu"></i> </a><a class="basebutton" aria-label="控制中心" id="popbutton" role="button"><i class="iconfont button-icon icon-expand"></i> </a><a class="basebutton darkwidget" aria-label="夜色模式" id="darkbutton" role="button"><i class="iconfont button-icon icon-weather"></i> </a><a class="basebutton searchwidget" aria-label="搜索" id="searchbutton" role="button"><i class="iconfont button-icon icon-search"></i></a><script>var addImgLayout=function(){for(var t=document.querySelectorAll(".post-content img"),e=0;e<t.length;e++){var a=document.createElement("a"),l=(a.setAttribute("href",t[e].getAttribute("data-src")),a.setAttribute("aria-label","illustration"),a.style.cssText="width: 100%; display: flex; justify-content: center;",t[e].alt&&(a.dataset.caption=t[e].alt),a.dataset.nolink=!0,t[e].before(a),a.append(t[e]),document.createElement("div"));l.classList.add("gallery"),a.before(l),l.append(a)}baguetteBox.run(".gallery")}</script><script>loadScript("/js/lib/lightbox/baguetteBox.min.js",addImgLayout)</script><script src="/js/main.js"></script><script>loadScript("/js/lib/busuanzi.min.js")</script><script>var addLazyload=function(){lozad(".lozad",{load:function(a){a.srcset=a.getAttribute("data-src")},loaded:function(a){a.classList.add("loaded")}}).observe()}</script><script>loadScript("/js/lib/lozad.min.js",addLazyload)</script><script src="//instant.page/5.1.0" integrity="sha384-by67kQnR+pyfy8yWP4kPO12fHKRLHZPfEsiSXR8u2IKcTdxD805MGUXBzVPnkLHw" type="module"></script><script>setTimeout(()=>{localSearch("search.json")},0)</script><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({model:{jsonPath:"/live2dw/assets/assets/remu.model.json"},display:{superSample:3,width:200,height:400,position:"right",hOffset:60,vOffset:-30},mobile:{show:!0,scale:.5},react:{opacity:.8},log:!1,pluginJsPath:"lib/",pluginModelPath:"assets/",pluginRootPath:"live2dw/",tagMode:!1})</script></body></html>